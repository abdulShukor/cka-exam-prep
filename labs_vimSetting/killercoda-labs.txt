
Home dir vs root dir 

Crash API server take backup
cp /etc/kubernetes/manifests/kube-apiserver.yaml ~/kube-apiserver.yaml.ori

NOTES:
In netpol 
 Won't work. Need to give two space to match labels cause its child of matchLabels
- namespaceSelector:
  matchLabels:
    kubernetes.io/metadata.name: space2 // its child of matchLabels

To Port: <any> (traffic allowed to all ports):
We don't put port section in the yaml 
Default to all ports 
- ports vs ports in netpol 

Vim:
 ~/.vimrc is used for global Vim settings that apply to all sessions launched by a user, while .vimrc in a specific directory is used for project-specific settings that only apply to sessions launched from that directory. The settings in the local .vimrc file override those in the global ~/.vimrc file when conflicts arise.

ClusterRole, by contrast, is a non-namespaced resource. The resources have different names (Role and ClusterRole) because a Kubernetes object always has to be either namespaced or not namespaced; it can't be both.
Cluster wide resource is accessible all over the cluster no namespaces involved 

Role + RoleBinding (available in single Namespace, applied in single Namespace)
ClusterRole + ClusterRoleBinding (available cluster-wide, applied cluster-wide)
ClusterRole + RoleBinding (available cluster-wide, applied in single Namespace)
Role + ClusterRoleBinding (NOT POSSIBLE: available in single Namespace, applied cluster-wide)

Log locations to check:
/var/log/pods
/var/log/containers
crictl ps + crictl logs
docker ps + docker logs (in case when Docker is used)
kubelet logs: /var/log/syslog or journalctl
journalctl | grep apiserver
journalctl | grep apiserver | grep err
watch crictl ps
journalctl | grep apiserver | grep failed
cat /var/log/syslog | grep kube-apiserver 
k logs deployments/collect-data -c nginx >> /root/logs.log 
System logs vs application logs 

# always make a backup !
cp /etc/kubernetes/manifests/kube-apiserver.yaml ~/kube-apiserver.yaml.ori
And because of the entry in /etc/hosts we can call
curl http://world.universe.mine:30080
Vi /etc/hosts 
172.30.1.2 world.universe.mine
We can reach the NodePort Service via the K8s Node IP:
k get nodes controlplane  -o wide  get the ip address of the control node then the ingress nodePort 
k -n ingress-nginx get svc ingress-nginx-controller
curl http://172.30.1.2:30080 -> control plan node ip then the ingress node-port 
k get ingressclass

If you specify both nodeSelector and nodeAffinity, both must be satisfied for the Pod to be scheduled onto a node.