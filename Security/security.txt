
------
1. ls /etc/kubernetes/manifests/
2. cat /etc/kubernetes/manifests/kube-apiserver.yaml 
3. cat /etc/kubernetes/manifests/etcd.yaml 
4. openssl x509 -in /etc/kubernetes/pki/apiserver.crt -text -noout
5. openssl x509 -in /etc/kubernetes/pki/etcd/server.crt -text -noout
6. openssl x509 -in /etc/kubernetes/pki/ca.crt  -text -noout. // root or ca certificate 
7. crictl  ps -a | grep kube-apiserver
8. crictl logs 6f707f569b572
9. crictl  ps -a | grep etcd
10. crictl logs 7e46aac2d97ec
11. ls /etc/kubernetes/pki/etcd/
12. cat /etc/kubernetes/manifests/etcd.yaml 
13. cat /etc/kubernetes/manifests/etcd.yaml | grep server-certificate.crt
14. vi  /etc/kubernetes/manifests/etcd.yaml
15. crictl ps -a | grep kube-apiserver
16. 

17. vi /etc/kubernetes/manifests/kube-apiserver.yaml // etcd and api server has its own ca certificate


Note:

/PKI dir // kubernetes  API sever 
/PKI/etcd.  For etcd
Etcd has its own CA and so the API sever had its own 

 ls /etc/kubernetes//pki/etcd/ca.crt 
 ls /etc/kubernetes//pki/etcd/
 vi /etc/kubernetes/manifests/kube-apiserver.yaml 
 - --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt


CERTIFICATES API
---

1. cat akshay.csr | base64 -w 0 // make sure print in single line 
2. cat > akshay.yaml
3. k certificate approve akshay
4. k get csr
5. k get csr agent-smith -o yaml
6. k certificate deny agent-smith
7. k delete csr agent-smith      

Note:
No imperative commands 

---
apiVersion: certificates.k8s.io/v1
kind: CertificateSigningRequest
metadata:
  name: akshay
spec:
  request: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURSBSRVFVRVNULS0tLS0KTUlJQ1ZqQ0NBVDRDQVFBd0VURVBNQTBHQTFVRUF3d0dZV3R6YUdGNU1JSUJJakFOQmdrcWhraUc5dzBCQVFFRgpBQU9DQVE4QU1JSUJDZ0tDQVFFQW9yL3JpbXlpYVJZL1RoSnlYRllYVm5UaC9MUmFOT0RCbWlJelYwNEtKQkFOClR6MlMzdDVhT25ZUWR5V0Zlc1pDZUFHeFpsR3JMQXE2eDdqNm9kbTgySjNOdU8yRWk3SUl0ZU5PeEhvdjZjUDUKalQxblFqV0dURlhsWDhmS3ZLTDQ2VXFtODN6STRPMFNkemRBSnJyWGdIL2NoL2QrbnNqMFZ1ejN3V0pOTWpBbwo3SGlhdlgzcnorVlduSTdIR1dLa0Z5T2dJUGxvRVRha1F1YVNyZEgwRUlUT0R2OVRZQmQ5VkdlYUN3V0tmb25kCk0xaVI1c2xPK1hoK2tNU1Z1V2lvNVJnTHFIYkpHeHhqUTVOa0ZPNnF3M3paYUJzNVB2R2NEdWFoNnpuMXZCN08KaExDYTFnWi9RVzYraXlCZU4yVk9rcmNXbDVXay84TGc3Skh3Z28zSVdRSURBUUFCb0FBd0RRWUpLb1pJaHZjTgpBUUVMQlFBRGdnRUJBSjFIMmR3WndMWTNqZXdodnVNV3VUbldIMm9BNjZFd3RJT0ZZcW1XRXc0WWkzdHU4dXZWCk5TT3JrZXZVZTQzMTY2TUNvclJZRUx6c09YVE9YYSs5RzlaTUY4eDJudWZqa0ozWFZCV3U4QU1ST0JVcDZaV3YKcFd1RnduNkt1dlVEYnpBa2g4dklJWTg3UHJtRzYrd1FGdzhmSi9vcmxXYjR5U2tTRlZESnA4YktEZ2plcDdxNQpLenUvTlc5bUw5dXVoQjM5RDJ6aFhqUVE5VHhVek9lc3ZSUXp5YmU1U3BNakY4bi9DZ1Q3QzY1cE1Tb25wUTkvCmFlSWh5YkZPWStMaUxqblkweC9oSUU1bDV2L1QrbExjUktNb213U1V1S1Z2aDZMWFhiQjg5c2NkdmRKWkNFY24KN3lmYjdCNmg2bTFMaXFVcGFzcUhWZHJSUi80RDJLclMzanM9Ci0tLS0tRU5EIENFUlRJRklDQVRFIFJFUVVFU1QtLS0tLQo=
  signerName: kubernetes.io/kube-apiserver-client
  usages:
  - client auth


KUBECONFIG:
---
1. cat .kube/config //root 
2. kubectl config use-context research --kubeconfig /root/my-kube-config
3. mv /root/my-kube-config  .kube/config 
4. k config view 
5. cat /root/.kube/config 
6. ls /etc/kubernetes/pki/users // all client cert 
7. ls /etc/kubernetes/pki/users/dev-user/
8. vi /root/.kube/config 

NOTE:
We can have multiple config file. 
.kube/config  I shte default config file 


RBAC:
--

1. cat /etc/kubernetes/manifests/kube-apiserver.yaml 
2.  - --authorization-mode=Node,RBAC
3. ps -aux | grep authorization
4. k get roles
5. k get roles -A --no-headers | wc -l
6. k describe role -n kube-system  kube-proxy 
7. k get rolebindings -n kube-system
8. k describe rolebindings -n kube-system kube-proxy
9. k get pods --as dev-user
10. k create role developer --verb=list,get,create,delete --resource=pods
11. k create rolebinding dev-user-binding --role=developer --user=dev-user
12. k --as dev-user get pod dark-blue-app -n blue
13. k edit role -n blue developer 
14. k edit role -n blue developer 
15. k describe role -n blue developer


NOTE:


- apiGroups:
  - "apps"
  resources:
  - deployments
  verbs:
  - get
  - watch
  - create
  - delete


- apiGroups:
  - "apps"
  resourceNames:
  - deployment
  resources:
  - deployments
  verbs:
  - get
  - watch
  - create
  - delete

CLUSTER:
---
1. k get clusterrole
2. k get clusterrole --no-headers | wc -l
3. k get clusterrolebindings --no-headers | wc -l
4. k describe clusterrolebindings cluster-admin 
5. k get clusterrolebindings | grep cluster-admin
6. k describe clusterrole cluster-admin 
7. k get nodes --as michelle
8. k create clusterrole  michelle-role --verb=get,list,watch --resource=nodes
9. k create clusterrolebinding michelle-role-binding --clusterrole=michelle-role --user=michelle
10. k describe clusterrole michelle-role 
11.  k describe clusterrolebindings.rbac.authorization.k8s.io  michelle-role-binding 
12. k api-resources 
13. k create clusterrole storage-admin --resource=persistentvolumes,storageclasses --verb=list,create,get,watch
14. k get clusterrole storage-adminn -o yaml
15. k create clusterrolebinding michelle-storage-admin --user=michelle --clusterrole=storage-admin

NOTE:
Cluster role binding to a user or group(list of user). User can be a service account. 



Service Account:
---
1. k get sa
2. k describe sa default 
3. k create serviceaccount dashboard-sa
4. ls /var/rbac
5. kubectl create token dashboard-sa

NOTE:
Under pod spec:  
serviceAccountName: dashboard-sa
Best practice to get the output into yaml when we create k8 abject through create not apply command

Securing Images: 
--->

1. k create secret.   //docker registry 
2. kubectl create secret docker-registry private-reg-cred  --docker-server=myprivateregistry.com:5000  --docker-username=dock_user  --docker-password=dock_password   --docker-email=dock_user@myprivateregistry.com
3. k edit deployments web 

Notes
 spec:
      containers:
      - image:myprivateregistry.com:5000/nginx:alpine
---
:wq!  Forceful writing 
---
  imagePullSecrets:
  - name: regcred
---
    spec:
      imagePullSecrets:
      - name: private-reg-cred


Security Context:
---

1.  whoami
2. k exec ubuntu-sleeper -- whoami
3. / to search in vim 
4. k get pods ubuntu-sleeper -o yaml > ub.yaml // getting in yaml file make it easier for further updates. We have the configuration. 
5. k delete pod ubuntu-sleeper --force
6. 


Note:
  securityContext: 
    runAsUser: 1010
--
spec:
  securityContext: // pod level 
    runAsUser: 1001
  containers:
  -  image: ubuntu
     name: web
     command: ["sleep", "5000"]
     securityContext:    / container level 
      runAsUser: 1002
--
    securityContext:
      capabilities:
        add: ["SYS_TIME"]

Network Policies:

1. k get networkpolicies
2. k describe netpol payroll-policy
3. k describe netpol internal-policy 


Note:
We cannot ping Payroll pod from internal pod 
ingress incoming traffic and egress is outgoing traffic from pod

Lab
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name:  internal-policy
  namespace: default
spec:
  podSelector:
    matchLabels:
      name: internal
  policyTypes:
    - Egress
  egress:
   - to:
       - podSelector:
           matchLabels:
             name: payroll
     ports:
       - protocol: TCP
         port: 8080
   - to:
        - podSelector:
            matchLabels:
              name: mysql
     ports:
       - protocol: TCP
         port: 3306